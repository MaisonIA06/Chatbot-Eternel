"""
Chatbot √âternel - MIA La Maison de l'IA
Serveur Flask avec int√©gration LM Studio
Th√®me : Grandes figures de l'histoire de la m√©decine et des sciences
"""

from flask import Flask, render_template, request, jsonify, session
from openai import OpenAI
import requests
import os
import secrets

app = Flask(__name__)

# Cl√© secr√®te pour les sessions Flask (g√©n√©r√©e automatiquement)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', secrets.token_hex(32))

# ============================================================
# CONFIGURATION - Modifiez ces valeurs selon votre setup
# ============================================================
LM_STUDIO_URL = os.environ.get('LM_STUDIO_URL', "http://localhost:1234/v1")
LM_STUDIO_API_KEY = os.environ.get('LM_STUDIO_API_KEY', "lm-studio")

# Nom du mod√®le charg√© dans LM Studio
# Mistral 7B Instruct v0.2 - Excellent en fran√ßais, rapide sur RTX 3060
MODEL_NAME = os.environ.get('MODEL_NAME', "mistral-7b-instruct-v0.2")

# Temp√©rature de g√©n√©ration (0.0 = d√©terministe, 1.0 = cr√©atif)
TEMPERATURE = float(os.environ.get('TEMPERATURE', '0.2'))

# Nombre maximum de tokens par r√©ponse (limite la longueur des messages)
MAX_TOKENS = int(os.environ.get('MAX_TOKENS', '100'))

# Nombre maximum de messages dans l'historique (pour limiter les tokens)
MAX_HISTORY_LENGTH = int(os.environ.get('MAX_HISTORY_LENGTH', '10'))

# Mode debug (d√©sactiver en production)
DEBUG_MODE = os.environ.get('FLASK_DEBUG', 'false').lower() == 'true'

# ============================================================
# PERSONNAGES - Grandes figures de la m√©decine et des sciences
# ============================================================

ROLES_GROUP_1 = {
    "Hildegarde de Bingen": """tu es Hildegarde de Bingen, abbesse b√©n√©dictine allemande du XIIe si√®cle (1098-1179), 
        visionnaire, compositrice, naturaliste et m√©decin. Tu as √©crit le "Liber Subtilitatum" sur les propri√©t√©s 
        curatives des plantes, pierres et animaux. Tu crois en l'harmonie entre le corps, l'√¢me et le cosmos. 
        Tu parles avec sagesse mystique, citant parfois tes visions divines. Tu utilises les termes "viriditas" 
        (force vitale verte) et "humeurs". Tu fais des r√©ponses courtes et profondes.""",
    
    "Florence Nightingale": """tu es Florence Nightingale, infirmi√®re britannique (1820-1910), pionni√®re des 
        soins infirmiers modernes. Tu as r√©volutionn√© l'hygi√®ne hospitali√®re pendant la guerre de Crim√©e, 
        r√©duisant drastiquement la mortalit√©. Tu es aussi statisticienne et as invent√© le diagramme polaire. 
        Tu parles avec d√©termination et compassion, insistant sur l'importance de l'hygi√®ne, de l'air frais, 
        de la lumi√®re et de la nutrition. Tu cites parfois tes "Notes on Nursing". Tu fais des r√©ponses courtes.""",
    
    "Marie Curie": """tu es Marie Curie, physicienne et chimiste franco-polonaise (1867-1934). N√©e Maria 
        Sk≈Çodowska √† Varsovie, tu as d√©couvert le polonium et le radium avec ton mari Pierre. Tu es la 
        premi√®re femme Prix Nobel et la seule personne √† avoir re√ßu deux Prix Nobel dans deux sciences 
        diff√©rentes (physique 1903, chimie 1911). Tu parles avec passion de la science, humilit√© et 
        d√©termination. Tu √©voques parfois Pierre et tes filles Ir√®ne et √àve. Tu fais des r√©ponses courtes.""",
    
    "Rosalind Franklin": """tu es Rosalind Franklin, chimiste et cristallographe britannique (1920-1958). 
        Tes travaux de diffraction aux rayons X, notamment la c√©l√®bre "Photo 51", ont √©t√© essentiels pour 
        d√©couvrir la structure en double h√©lice de l'ADN. Tu es rigoureuse, perfectionniste et passionn√©e 
        par la science exp√©rimentale. Tu parles avec pr√©cision scientifique et un certain agacement quand 
        on minimise ton travail. Tu mentionnes parfois ton travail sur les virus. Tu fais des r√©ponses courtes.""",
}

ROLES_GROUP_2 = {
    "Hippocrate": """tu es Hippocrate de Cos, m√©decin grec de l'Antiquit√© (vers 460-370 av. J.-C.), 
        consid√©r√© comme le p√®re de la m√©decine. Tu as fond√© l'√©cole de m√©decine de Cos et √©tabli la 
        m√©decine comme discipline rationnelle, s√©par√©e de la religion. Tu crois en la th√©orie des quatre 
        humeurs (sang, phlegme, bile jaune, bile noire). Tu parles avec sagesse et √©thique, citant parfois 
        ton serment. "Primum non nocere" - d'abord ne pas nuire. Tu fais des r√©ponses courtes et sages.""",
    
    "Avicenne": """tu es Ibn Sina, appel√© Avicenne en Occident, m√©decin et philosophe persan (980-1037). 
        Tu as √©crit le "Canon de la m√©decine", ouvrage de r√©f√©rence pendant des si√®cles en Europe et dans 
        le monde islamique. Tu ma√Ætrises la philosophie d'Aristote, l'astronomie, les math√©matiques et la 
        po√©sie. Tu parles avec √©rudition, m√™lant m√©decine et philosophie. Tu √©voques parfois Galien, 
        Aristote ou tes voyages √† travers la Perse. Tu fais des r√©ponses courtes et savantes.""",
    
    "Louis Pasteur": """tu es Louis Pasteur, chimiste et physicien fran√ßais (1822-1895), pionnier de la 
        microbiologie. Tu as d√©couvert les principes de la vaccination, de la pasteurisation et r√©fut√© 
        la g√©n√©ration spontan√©e. Tu as sauv√© l'industrie de la soie et cr√©√© le vaccin contre la rage. 
        Tu parles avec passion et conviction scientifique. Tu cites parfois "Le hasard ne favorise que 
        les esprits pr√©par√©s". Tu mentionnes ton laboratoire et tes exp√©riences. Tu fais des r√©ponses courtes.""",
    
    "Alexander Fleming": """tu es Alexander Fleming, m√©decin et biologiste √©cossais (1881-1955), 
        d√©couvreur de la p√©nicilline en 1928, Prix Nobel de m√©decine 1945. Cette d√©couverte accidentelle 
        d'une moisissure qui tue les bact√©ries a r√©volutionn√© la m√©decine. Tu parles avec humour britannique 
        et modestie, rappelant que "la chance favorise l'esprit pr√©par√©". Tu √©voques parfois St Mary's Hospital 
        √† Londres ou tes coll√®gues Florey et Chain. Tu fais des r√©ponses courtes et pragmatiques.""",
}

# Personnages par d√©faut
DEFAULT_ROLE1 = "Marie Curie"
DEFAULT_ROLE2 = "Louis Pasteur"


def check_lm_studio_connection():
    """V√©rifie que LM Studio est accessible"""
    try:
        response = requests.get(f"{LM_STUDIO_URL}/models", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Connexion LM Studio OK")
            return True
    except requests.exceptions.ConnectionError:
        print(f"‚ùå ERREUR: LM Studio n'est pas accessible sur {LM_STUDIO_URL}")
        print("   ‚Üí Lancez LM Studio avec: lms server start")
        return False
    except Exception as e:
        print(f"‚ùå ERREUR: {e}")
        return False
    return False


# Client OpenAI pointant vers LM Studio
client = OpenAI(base_url=LM_STUDIO_URL, api_key=LM_STUDIO_API_KEY)


def build_system_prompt(consigne):
    """Construit le prompt syst√®me pour le personnage"""
    return (
        f"Tu es {consigne} Tu r√©ponds UNIQUEMENT en fran√ßais, en 1√®re personne. "
        f"Ne dis jamais que tu es un mod√®le de langage ou une IA, reste absolument dans ton r√¥le historique. "
        f"Tu parleras de m√©decine, de science, de tes d√©couvertes, de ton √©poque, de philosophie et de la vie. "
        f"Tu poseras des questions √† ton interlocuteur sur ses propres travaux et d√©couvertes. "
        f"Tu peux partager des anecdotes de ta vie, √©voquer tes coll√®gues, tes d√©fis et tes r√©ussites. "
        f"Les √©changes peuvent √™tre amicaux, passionn√©s, ou m√™me comporter des d√©saccords scientifiques ou philosophiques. "
        f"Tu fais des r√©ponses TR√àS courtes d'une seule phrase, maximum 15 mots. "
        f"Ne signe JAMAIS tes messages, ne mets pas ton nom √† la fin."
    )


def get_session_data():
    """R√©cup√®re ou initialise les donn√©es de session"""
    if 'role1' not in session:
        session['role1'] = DEFAULT_ROLE1
        session['consigne1'] = ROLES_GROUP_1[DEFAULT_ROLE1]
    if 'role2' not in session:
        session['role2'] = DEFAULT_ROLE2
        session['consigne2'] = ROLES_GROUP_2[DEFAULT_ROLE2]
    if 'conversation_history' not in session:
        session['conversation_history'] = []
    
    return {
        'role1': session['role1'],
        'role2': session['role2'],
        'consigne1': session['consigne1'],
        'consigne2': session['consigne2'],
        'conversation_history': session['conversation_history']
    }


def generate_response(message, consigne, speaker_role):
    """G√©n√®re une r√©ponse via LM Studio avec gestion d'erreurs et historique"""
    data = get_session_data()
    conversation_history = data['conversation_history']
    
    print(f"üé≠ Personnage: {speaker_role}")
    print(f"üìù Historique: {len(conversation_history)} messages")
    
    # Construire les messages (sans r√¥le "system" car non support√© par certains mod√®les)
    messages = []
    
    # Ajouter l'historique (limit√© aux N derniers messages)
    messages.extend(conversation_history[-MAX_HISTORY_LENGTH:])
    
    # Construire le message utilisateur avec les instructions du personnage int√©gr√©es
    system_instructions = build_system_prompt(consigne)
    if message:
        # Int√©grer les instructions dans le message utilisateur
        full_message = f"[Instructions: {system_instructions}]\n\nMessage de ton interlocuteur: {message}"
        messages.append({"role": "user", "content": full_message})
    else:
        # Premier message - juste les instructions pour d√©marrer
        messages.append({"role": "user", "content": f"[Instructions: {system_instructions}]\n\nCommence la conversation en te pr√©sentant bri√®vement."})
    
    try:
        completion = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
        )
        response = completion.choices[0].message.content
        
        # Ajouter √† l'historique de session
        if message:
            conversation_history.append({"role": "user", "content": message})
        conversation_history.append({"role": "assistant", "content": response})
        
        # Limiter la taille de l'historique
        if len(conversation_history) > MAX_HISTORY_LENGTH * 2:
            conversation_history = conversation_history[-MAX_HISTORY_LENGTH:]
        
        # Sauvegarder dans la session
        session['conversation_history'] = conversation_history
        session.modified = True
        
        return response
        
    except Exception as e:
        error_msg = f"Erreur de g√©n√©ration: {str(e)}"
        print(f"‚ùå {error_msg}")
        return "[Erreur: LM Studio ne r√©pond pas. V√©rifiez que le serveur est lanc√©.]"


@app.route("/")
def home():
    """Page d'accueil avec le chatbot"""
    data = get_session_data()
    return render_template("index.html", role1=data['role1'], role2=data['role2'])


@app.route("/update_characters", methods=["POST"])
def update_characters():
    """Met √† jour les personnages s√©lectionn√©s"""
    role1 = request.form.get("role1", DEFAULT_ROLE1)
    role2 = request.form.get("role2", DEFAULT_ROLE2)
    
    # Valider les r√¥les
    if role1 not in ROLES_GROUP_1:
        role1 = DEFAULT_ROLE1
    if role2 not in ROLES_GROUP_2:
        role2 = DEFAULT_ROLE2
    
    # Mettre √† jour la session
    session['role1'] = role1
    session['role2'] = role2
    session['consigne1'] = ROLES_GROUP_1[role1]
    session['consigne2'] = ROLES_GROUP_2[role2]
    session['conversation_history'] = []  # R√©initialiser l'historique
    session.modified = True
    
    print(f"üîÑ Nouvelle conversation: {role1} ‚Üî {role2}")

    return jsonify({
        "role1": role1,
        "role2": role2,
        "consigne1": session['consigne1'],
        "consigne2": session['consigne2']
    })


@app.route("/get_response", methods=["POST"])
def get_response():
    """G√©n√®re une r√©ponse du personnage"""
    message = request.form.get("message", "")
    role = request.form.get("role", "")
    
    data = get_session_data()
    
    # D√©terminer qui doit r√©pondre
    if role == data['role2']:
        new_role = data['role1']
        new_consigne = data['consigne1']
    else:
        new_role = data['role2']
        new_consigne = data['consigne2']
    
    response = generate_response(message, new_consigne, new_role)
    
    return jsonify({
        "response": response,
        "role": new_role
    })


@app.route("/health")
def health_check():
    """Endpoint de v√©rification de l'√©tat du serveur"""
    lm_studio_ok = check_lm_studio_connection()
    return jsonify({
        "status": "ok",
        "lm_studio_connected": lm_studio_ok,
        "model": MODEL_NAME
    })


if __name__ == "__main__":
    print("=" * 60)
    print("üè• Chatbot √âternel - MIA La Maison de l'IA")
    print("   Th√®me : Grandes figures de la m√©decine")
    print("=" * 60)
    
    # V√©rifier la connexion LM Studio au d√©marrage
    if not check_lm_studio_connection():
        print("\n‚ö†Ô∏è  D√©marrage sans LM Studio - les r√©ponses seront en erreur")
        print("   Pour d√©marrer LM Studio: lms server start && lms load <mod√®le>\n")
    
    print(f"\nüìä Configuration:")
    print(f"   ‚Ä¢ Mod√®le: {MODEL_NAME}")
    print(f"   ‚Ä¢ Temp√©rature: {TEMPERATURE}")
    print(f"   ‚Ä¢ Max tokens: {MAX_TOKENS}")
    print(f"   ‚Ä¢ Historique max: {MAX_HISTORY_LENGTH} messages")
    print(f"   ‚Ä¢ Mode debug: {DEBUG_MODE}")
    print(f"\nüë• Personnages disponibles:")
    print(f"   Groupe 1: {', '.join(ROLES_GROUP_1.keys())}")
    print(f"   Groupe 2: {', '.join(ROLES_GROUP_2.keys())}")
    print(f"\nüåê Serveur: http://localhost:5000")
    print("=" * 60 + "\n")
    
    app.run(debug=DEBUG_MODE, host='0.0.0.0', port=5000)
